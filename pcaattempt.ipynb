{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Loads data from csv file\n",
    "#   filename - string of the file name\n",
    "def loadCsv(filename):\n",
    "    lines = csv.reader(open(filename, newline=''), delimiter=',', quotechar='|')\n",
    "    next(lines)\n",
    "    dataset = []\n",
    "    for row in lines:\n",
    "        dataset.append([float(x) for x in row])\n",
    "    return dataset\n",
    "\n",
    "# Splits data into training and validation portions\n",
    "#   dataset - matrix of the whole dataset\n",
    "#   splitRatio - float of the ratio\n",
    "def splitDataset(dataset, splitRatio):\n",
    "    trainSize = int(len(dataset) * splitRatio)\n",
    "    trainSet = []\n",
    "    copy = list(dataset)\n",
    "    while len(trainSet) < trainSize:\n",
    "        index = random.randrange(len(copy))\n",
    "        trainSet.append(copy.pop(index))\n",
    "    return [trainSet, copy]\n",
    "\n",
    "# Oversamples the minority data\n",
    "def oversampling(trainSet):\n",
    "    labels = list(zip(*[reversed(el) for el in trainSet]))[0]\n",
    "    label0 = np.array([i for i in trainSet if i[-1] == 0])\n",
    "    label1 = np.array([j for j in trainSet if j[-1] == 1])\n",
    "    a = len(label0)\n",
    "    b = len(label1)\n",
    "    if a > b:\n",
    "        factor = a // b\n",
    "        if factor > 1:\n",
    "            factor -= 1\n",
    "        label1 = np.repeat(label1, repeats=factor, axis=0)\n",
    "    elif a < b:\n",
    "        factor = b // a\n",
    "        if factor > 1:\n",
    "            factor -= 1\n",
    "        label0 = np.repeat(label0, repeats=factor, axis=0)\n",
    "    print(label0.shape)\n",
    "    print(label1.shape)\n",
    "    new_set = np.concatenate([label0,label1])\n",
    "    return new_set\n",
    "\n",
    "# Initializes a np.array of weights of 0 with weights[-1] being w_0\n",
    "#   trainSet - matrix of the training set\n",
    "def initializeWeights(trainSet):\n",
    "    weights = np.zeros(len(trainSet[0]))\n",
    "    return weights\n",
    "\n",
    "# Sigmoid function\n",
    "#   a - number to plug into the sigmoid function\n",
    "def sigmoid(a):\n",
    "    return (1.0 / (1 + np.exp(-a)))\n",
    "\n",
    "# Returns the prediction probability for one individual\n",
    "#   features - list of the features' values for one patient\n",
    "#   weights - np.array of the weights\n",
    "def predict(features, weights):\n",
    "    a = np.dot(np.append(features,np.array([1])), weights)\n",
    "    return sigmoid(a)\n",
    "\n",
    "# Returns a list of prediction probabilities of all individuals\n",
    "def getPredictions(weights, trainSet):\n",
    "    predictions = []\n",
    "    for t in trainSet:\n",
    "        p = predict(t[:-1],weights)\n",
    "        predictions.append(p)\n",
    "    return predictions\n",
    "\n",
    "# Updates the weights after each batch\n",
    "#   predicted - (x,1) array of the predicted labels\n",
    "#   labels - (x,1) array of the actual labels from the data\n",
    "#   weights - (1,69) array of weights\n",
    "#   lr - learning rate\n",
    "def gradientDescent(batch, predicted, labels, weights, lr):\n",
    "    bias_update = 0\n",
    "    add_weights = np.zeros(len(weights)-1)\n",
    "    for i in range(len(batch)):\n",
    "        y_hat = predicted[i]\n",
    "        y_i = labels[i]\n",
    "        bias_update += ((y_hat-y_i)*(y_hat*(1-y_hat)))\n",
    "        add_weights += ((y_hat-y_i)*(y_hat*(1-y_hat)))*batch[i][:-1]\n",
    "    add_weights = np.append(add_weights,np.array([bias_update]))\n",
    "    add_weights /= len(batch)\n",
    "    add_weights *= lr\n",
    "    weights -= add_weights\n",
    "    return weights\n",
    "\n",
    "# Calculates the cross entropy loss\n",
    "#   batch - (x,69) matrix of batch size of x individuals \n",
    "def crossEntropyLoss(weights, batch):\n",
    "    y_true = [t[-1] for t in batch]\n",
    "    y_predict = getPredictions(weights, batch)\n",
    "    num_data = len(batch)\n",
    "    total = 0\n",
    "    for i in range(len(batch)):\n",
    "        if y_true[i] == 1:\n",
    "            total += -np.log(y_predict[i])\n",
    "        else:\n",
    "            total += -np.log(1-y_predict[i])\n",
    "    loss = total / num_data\n",
    "    return loss\n",
    "\n",
    "# Splits the training set into batches of equal to near equal size\n",
    "#   batchSize - int for the size of the batch\n",
    "def splitIntoBatches(trainSet, batchSize):\n",
    "    trainSet = np.array(trainSet)\n",
    "    batches = np.array_split(trainSet, batchSize)\n",
    "    return batches\n",
    "\n",
    "# Trains the model\n",
    "#   nEpoch - int for number of epochs\n",
    "def train(weights, trainSet, batchSize, nEpoch, lr):\n",
    "    batches = splitIntoBatches(trainSet, batchSize)\n",
    "    losses = []\n",
    "    epochs = []\n",
    "    loss_plot = []\n",
    "    for i in range(nEpoch):\n",
    "        for b in batches:\n",
    "            predicted = getPredictions(weights, b)\n",
    "            new_weights = gradientDescent(b, predicted, b[:,-1], weights, lr)\n",
    "            loss = crossEntropyLoss(new_weights, b)\n",
    "            losses.append(loss)\n",
    "        # Checkpoint: prints out the loss every 50 epochs\n",
    "        if i % 50 == 0:\n",
    "            print (\"iter: \" + str(i) + \" loss: \" + str(loss))\n",
    "            epochs.append(i)\n",
    "            loss_plot.append(loss)\n",
    "#     plt.plot(epochs, loss_plot)\n",
    "#     plt.show()\n",
    "    return new_weights, losses\n",
    "\n",
    "# Calculates the total accuracy and accuracies of each class\n",
    "def accuracy(labels, predicted):\n",
    "    total = 0\n",
    "    total_0 = 0\n",
    "    total_1 = 0\n",
    "    for i in range(len(predicted)):\n",
    "        p = predicted[i]\n",
    "        a = labels[i][-1]\n",
    "        if p == a:\n",
    "            total += 1\n",
    "            if p == 0 and a == 0:\n",
    "                total_0 += 1\n",
    "            elif p == 1 and a == 1:\n",
    "                total_1 += 1\n",
    "    acc = total / len(predicted)\n",
    "    acc_0 = total_0 / list(list(zip(*map(reversed, labels)))[0]).count(0)\n",
    "    acc_1 = total_1 / list(list(zip(*map(reversed, labels)))[0]).count(1)\n",
    "    output = \"Total accuracy = \" + str(round(acc,4))\n",
    "    output += \"\\nClass 0 accuracy = \" + str(round(acc_0,4))\n",
    "    output += \"\\nClass 1 accuracy = \" + str(round(acc_1,4))\n",
    "    print(output)\n",
    "\n",
    "# Computes performance in terms of F1 scores\n",
    "#   predictions - (x,1) array of the predicted labels\n",
    "def evaluate(testSet, predictions):\n",
    "    tp1 = 0.0\n",
    "    fp1 = 0.0\n",
    "    fn1 = 0.0\n",
    "    tp0 = 0.0\n",
    "    fp0 = 0.0\n",
    "    fn0 = 0.0\n",
    "    for i in range(len(testSet)):\n",
    "        if testSet[i][-1] == 0:\n",
    "            if predictions[i] == 0:\n",
    "                tp0 += 1.0\n",
    "            else:\n",
    "                fp1 += 1.0\n",
    "                fn0 += 1.0\n",
    "        else:\n",
    "            if predictions[i] == 1:\n",
    "                tp1 += 1.0\n",
    "            else:\n",
    "                fp0 += 1.0\n",
    "                fn1 += 1.0\n",
    "    p0 = tp0/(tp0+fp0)\n",
    "    p1 = tp1/(tp1+fp1)\n",
    "    r0 = tp0/(tp0+fn0)\n",
    "    r1 = tp1/(tp1+fn1)\n",
    "    print(\"F1 (Class 0): \" + str((2.0*p0*r0)/(p0+r0)))\n",
    "    print(\"F1 (Class 1): \" + str((2.0*p1*r1)/(p1+r1)))\n",
    "\n",
    "def main():\n",
    "    random.seed(13)\n",
    "    filename = 'readmission.csv'\n",
    "    splitRatio = 0.80\n",
    "    dataset = loadCsv(filename)\n",
    "    trainingSet, valSet = splitDataset(dataset, splitRatio)\n",
    "    print('Split ' + str(len(dataset)) + ' rows into train = ' + str(len(trainingSet)) \n",
    "          + ' and test = ' + str(len(valSet)) + ' rows.\\n')\n",
    "\n",
    "    pca = PCA(n_components = 50)\n",
    "    trainingSet2 = [i[:-1] for i in trainingSet]\n",
    "    pca.fit(trainingSet2)\n",
    "    reduced_training = pca.transform(trainingSet2)\n",
    "    valSet2 = [j[:-1] for j in valSet]\n",
    "    reduced_val = pca.transform(valSet2)\n",
    "    \n",
    "    print(len(reduced_training))\n",
    "    for i in range(len(reduced_training)):\n",
    "        reduced_training[i][-1] = int(round(trainingSet[i][-1]))\n",
    "        \n",
    "    for i in range(len(reduced_val)):\n",
    "        reduced_val[i][-1] = int(round(valSet[i][-1]))\n",
    "\n",
    "    reduced_training = oversampling(reduced_training)\n",
    "    random.shuffle(reduced_training)\n",
    "    \n",
    "#     plt.figure()\n",
    "#     plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "#     plt.xlabel('Number of Components')\n",
    "#     plt.ylabel('Variance (%)') \n",
    "#     plt.title('Explained Variance')\n",
    "#     plt.show()\n",
    "#     how does the pca know what the correct labels are?\n",
    "\n",
    "#     when doing this ^, do PCA(n_components = 68)\n",
    "#     used to find the proper number of components to keep. Variance loss begins at about lower than 50 components, \n",
    "#     so we picked ~50 components for pca to minimize info loss during dimensionality reduction\n",
    "    \n",
    "    weights = initializeWeights(reduced_training)\n",
    "    batch_size = 1200\n",
    "    n_epoch = 500\n",
    "    learning_rate = 0.0015\n",
    "    finalWeights, losses = train(weights, reduced_training, batch_size, n_epoch, learning_rate)\n",
    "    print(finalWeights)\n",
    "    valSet_before = getPredictions(finalWeights, reduced_val)\n",
    "    valSet_after = [0 if i < 0.5 else 1 for i in valSet_before]\n",
    "\n",
    "    # Test model\n",
    "    accuracy(reduced_val, valSet_after)\n",
    "    evaluate(reduced_val, valSet_after)\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
